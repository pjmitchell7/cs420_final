{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn opencv-python dlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import dlib\n",
    "import cv2\n",
    "import os\n",
    "import sqlite3\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from skimage.feature import hog\n",
    "import re\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# Function to load the embedding model\n",
    "def load_embedding_model():\n",
    "    model = ResNet50(weights='imagenet')\n",
    "    model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    return model\n",
    "\n",
    "# Load model, detector, and predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "embedding_model = load_embedding_model() \n",
    "\n",
    "def preprocess_input(image):\n",
    "    \"\"\"Ensure this preprocessing is the same as used in the training phase.\"\"\"\n",
    "    if image.ndim == 2 or (image.ndim == 3 and image.shape[2] == 1):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    image = image.astype('float32')\n",
    "    mean = [123.68, 116.779, 103.939]\n",
    "    image -= mean\n",
    "    image /= 255.0\n",
    "    image = cv2.resize(image, (224, 224))  # Ensure dimensions are consistent\n",
    "    return image\n",
    "\n",
    "def process_frame(frame):\n",
    "    \"\"\"Using the preprocessing during inference.\"\"\"\n",
    "    preprocessed_frame = preprocess_input(frame)\n",
    "    embedding = embedding_model.predict(np.expand_dims(preprocessed_frame, axis=0))\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def extract_components_from_face(image, landmarks):\n",
    "    COMPONENTS = {\n",
    "        'left_eye': range(36, 42),\n",
    "        'right_eye': range(42, 48),\n",
    "        'left_eyebrow': range(17, 22),\n",
    "        'right_eyebrow': range(22, 27),\n",
    "        'nose_bridge': range(27, 30),\n",
    "        'full_nose': range(27, 36),\n",
    "        'mouth': range(48, 68),\n",
    "    }\n",
    "    components = {}\n",
    "    for component, indices in COMPONENTS.items():\n",
    "        points = np.array([[landmarks.part(i).x, landmarks.part(i).y] for i in indices])\n",
    "        if points.size == 0:\n",
    "            continue  # Skip if no points are found\n",
    "        x, y, w, h = cv2.boundingRect(points)\n",
    "        if w > 0 and h > 0 and (y+h <= image.shape[0]) and (x+w <= image.shape[1]):\n",
    "            component_img = image[y:y+h, x:x+w]\n",
    "            if component_img.size > 0:  # Additional check for non-empty images\n",
    "                processed_img = preprocess_input(component_img)\n",
    "                components[component] = processed_img\n",
    "\n",
    "    return components\n",
    "\n",
    "\n",
    "\n",
    "def process_video(video_path, user_id, db_connection, label):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    selected_frames = set(np.linspace(0, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1, num=100, dtype=int))\n",
    "\n",
    "    frame_idx = 0\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_idx in selected_frames:\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = detector(gray)\n",
    "\n",
    "                for face in faces:\n",
    "                    landmarks = predictor(gray, face)\n",
    "                    components = extract_components_from_face(gray, landmarks)\n",
    "\n",
    "                    for component_name, component_img in components.items():\n",
    "                        if component_img is not None and component_img.size > 0:\n",
    "                            processed_img = preprocess_input(component_img)  # Preprocess the image\n",
    "                            embedding = embedding_model.predict(np.expand_dims(processed_img, axis=0))\n",
    "                            store_component_embedding(db_connection, user_id, component_name, embedding, label)\n",
    "\n",
    "            frame_idx += 1\n",
    "            if frame_idx >= max(selected_frames):\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "\n",
    "def store_component_embedding(db_connection, user_id, component, data, label):\n",
    "    cursor = db_connection.cursor()\n",
    "    try:\n",
    "        # Convert the embedding data to a high precision string representation\n",
    "        data_str = json.dumps(data.tolist(), separators=(',', ':'), ensure_ascii=False)\n",
    "        \n",
    "        # Insert data into SQLite database\n",
    "        cursor.execute('''\n",
    "            INSERT INTO component_embeddings (user_id, component, embedding, label)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        ''', (user_id, component, data_str, label))\n",
    "        db_connection.commit()\n",
    "\n",
    "        # log the data to a text file for verification\n",
    "        with open('output.txt', 'a') as f:\n",
    "            f.write(f\"{user_id}, {component}, {data_str}, {label}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data into component_embeddings: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "with sqlite3.connect('face_features.db') as conn:\n",
    "    video_directory = '/Users/paulmitchell/Downloads/CS420_Project/profiles'\n",
    "    video_files = [f for f in os.listdir(video_directory) if f.endswith('.mp4')]\n",
    "    for video_file in video_files:\n",
    "        user_id = video_file.split('.')[0]\n",
    "        process_video(os.path.join(video_directory, video_file), user_id, conn, label=1)\n",
    "    video_directory = '/Users/paulmitchell/Downloads/CS420_Project/unauth'\n",
    "    video_files = [f for f in os.listdir(video_directory) if f.endswith('.mp4')]\n",
    "    for video_file in video_files:\n",
    "        user_id = video_file.split('.')[0]\n",
    "        process_video(os.path.join(video_directory, video_file), user_id, conn, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "import json\n",
    "\n",
    "def parse_embedding(embedding_str):\n",
    "    # Load the embedding from a JSON-formatted string\n",
    "    embedding_vals = json.loads(embedding_str)\n",
    "    return embedding_vals\n",
    "\n",
    "\n",
    "def display_embeddings(embeddings):\n",
    "    \"\"\"Displays the embedding values for debugging.\"\"\"\n",
    "    for i, embedding in enumerate(embeddings, start=1):\n",
    "        print(f\"Embedding {i}: {embedding[:5]}...{embedding[-5:]} (Total {len(embedding)} values)\")\n",
    "\n",
    "def preprocess_embeddings(embeddings, component):\n",
    "    processed_embeddings = []\n",
    "    for embedding in embeddings:\n",
    "        try:\n",
    "            # Ensure embedding is a JSON string and parse it\n",
    "            embedding_vals = parse_embedding(embedding)\n",
    "            if embedding_vals:  # Ensure list is not empty\n",
    "                embed_array = np.array(embedding_vals, dtype=float)\n",
    "\n",
    "                # Normalize all component embeddings if not empty\n",
    "                embed_array -= np.mean(embed_array)\n",
    "                embed_array /= np.std(embed_array)\n",
    "                processed_embeddings.append(embed_array)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing embedding: {embedding} with error {e}\")\n",
    "\n",
    "    return np.array(processed_embeddings)\n",
    "\n",
    "\n",
    "def validate_embeddings(db_connection):\n",
    "    cursor = db_connection.cursor()\n",
    "    cursor.execute(\"SELECT data_id, component, embedding FROM component_embeddings\")\n",
    "    records = cursor.fetchall()\n",
    "\n",
    "    invalid_data_points = []\n",
    "    for record in records:\n",
    "        data_id, component, embedding_str = record\n",
    "        try:\n",
    "            embedding_vals = parse_embedding(embedding_str)\n",
    "            if not embedding_vals:  # Check if the list is empty\n",
    "                raise ValueError(\"No embedding values parsed.\")\n",
    "            \n",
    "            display_embeddings([embedding_vals])  # Debugging line to display embeddings\n",
    "\n",
    "            embedding_array = np.array(embedding_vals)\n",
    "            if embedding_array.size == 0:\n",
    "                raise ValueError(\"Empty embedding array after processing.\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing data point {data_id} ({component}): {e}\")\n",
    "            invalid_data_points.append(data_id)\n",
    "\n",
    "    if invalid_data_points:\n",
    "        print(f\"Validation found {len(invalid_data_points)} invalid entries. Consider reviewing and cleaning these entries.\")\n",
    "    else:\n",
    "        print(\"No invalid entries detected. Data integrity looks good.\")\n",
    "\n",
    "    return invalid_data_points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    db_connection = sqlite3.connect('face_features.db')\n",
    "    try:\n",
    "        validate_embeddings(db_connection)\n",
    "    finally:\n",
    "        db_connection.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(db_connection, component):\n",
    "    cursor = db_connection.cursor()\n",
    "    cursor.execute(\"SELECT embedding, label FROM component_embeddings WHERE component = ?\", (component,))\n",
    "    data = cursor.fetchall()\n",
    "    embeddings = [json.loads(record[0]) for record in data]\n",
    "    labels = [record[1] for record in data]\n",
    "    return embeddings, labels\n",
    "\n",
    "def preprocess_embeddings(embeddings):\n",
    "    processed_embeddings = []\n",
    "    for embedding in embeddings:\n",
    "        embedding_vals = np.array(embedding, dtype=float)\n",
    "        if embedding_vals.ndim > 1:\n",
    "            embedding_vals = embedding_vals.flatten()\n",
    "        processed_embeddings.append(embedding_vals)\n",
    "    return np.array(processed_embeddings)\n",
    "\n",
    "def evaluate_svm_with_roc(db_path, component, C, gamma, kernel):\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        embeddings, labels = load_data(conn, component)\n",
    "        embeddings = preprocess_embeddings(embeddings)\n",
    "\n",
    "    # Convert embeddings and labels to numpy arrays\n",
    "    X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=int)  # Ensures labels are of integer type\n",
    "\n",
    "    model = make_pipeline(StandardScaler(), SVC(C=C, gamma=gamma, kernel=kernel, probability=True))\n",
    "    model.fit(X_train, y_train)  # Fit the model on the entire training dataset\n",
    "\n",
    "    # Calculate and print training accuracy\n",
    "    training_accuracy = model.score(X_train, y_train)\n",
    "    print(f'Training accuracy for {component}: {training_accuracy:.2f}')\n",
    "\n",
    "    # ROC Curve calculation for visualization\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, (train, test) in enumerate(cv.split(X_train, y_train)):\n",
    "        viz = RocCurveDisplay.from_estimator(model, X_train[test], y_train[test], alpha=0.3, lw=1, ax=ax)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "            label=r'Mean ROC (AUC = %0.2f ± %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=2, alpha=.8)\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver Operating Characteristic for \" + component)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save the model\n",
    "    model_directory = \"saved_models\"\n",
    "    os.makedirs(model_directory, exist_ok=True)\n",
    "    model_path = os.path.join(model_directory, f\"{component}_svm_model.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Model for {component} saved to {model_path}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of components to their optimal C values\n",
    "component_c_values = {\n",
    "    'left_eye': 0.1,\n",
    "    'right_eye': 0.1,\n",
    "    'nose_bridge': 10,\n",
    "    'full_nose': 50,\n",
    "    'left_eyebrow': 50,\n",
    "    'right_eyebrow': 1,\n",
    "    'mouth': 0.1\n",
    "}\n",
    "\n",
    "# Main execution block for training\n",
    "components = [\"left_eye\", \"right_eye\", \"nose_bridge\", \"full_nose\", \"left_eyebrow\", \"right_eyebrow\", \"mouth\"]\n",
    "models = {}\n",
    "db_path = 'face_features.db'\n",
    "\n",
    "for component in components:\n",
    "    # Retrieve the best C value for the current component\n",
    "    best_c = component_c_values[component]\n",
    "    print(f\"Training {component} with C={best_c}\")\n",
    "    # Train the model using the component-specific C value\n",
    "    model = evaluate_svm_with_roc(db_path, component, C=best_c, gamma='scale', kernel='rbf')\n",
    "    models[component] = model\n",
    "    print(f\"Model trained and stored for {component}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def component_specific_grid_search(db_path, component):\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        embeddings, labels = load_data(conn, component)\n",
    "        embeddings = preprocess_embeddings(embeddings)\n",
    "\n",
    "    # Pipeline setup\n",
    "    pipeline = make_pipeline(StandardScaler(), SVC(kernel='rbf', gamma='scale'))\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'svc__C': [0.1, 1, 10, 50, 100]\n",
    "    }\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(embeddings, labels)\n",
    "\n",
    "    # Output the best parameters and score\n",
    "    print(f\"Best C for {component}: {grid_search.best_params_['svc__C']}\")\n",
    "    print(f\"Best score for {component}: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Apply to all components\n",
    "components = [\"left_eye\", \"right_eye\", \"nose_bridge\", \"full_nose\", \"left_eyebrow\", \"right_eyebrow\", \"mouth\"]\n",
    "for component in components:\n",
    "    component_specific_grid_search(db_path, component)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# SVM training accuracies for different facial components\n",
    "components = ['Left Eye', 'Right Eye', 'Nose Bridge', 'Full Nose', 'Mouth', 'Left Eyebrow', 'Right Eyebrow']\n",
    "accuracies = [0.87, 0.86, 0.99, 1.00, 1.00, 0.99, 0.90]\n",
    "\n",
    "def plot_svm_accuracies(components, accuracies):\n",
    "    fig, ax = plt.subplots()\n",
    "    y_pos = np.arange(len(components))\n",
    "    ax.barh(y_pos, accuracies, align='center', color='skyblue')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(components)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Accuracy')\n",
    "    ax.set_title('SVM Training Accuracies for Facial Components')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_svm_accuracies(components, accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_svm_with_cross_validation(db_path, component, C, gamma, kernel):\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        embeddings, labels = load_data(conn, component)\n",
    "        embeddings = preprocess_embeddings(embeddings)\n",
    "\n",
    "    # Create a pipeline with standardization and SVM\n",
    "    pipeline = make_pipeline(StandardScaler(), SVC(C=C, gamma=gamma, kernel=kernel))\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, embeddings, labels, cv=5, scoring='accuracy')  # 5-fold CV\n",
    "    \n",
    "    # Print results in a dictionary format\n",
    "    formatted_scores = ', '.join([f\"{score:.2f}\" for score in cv_scores])\n",
    "    print(f\"'{component.title()}': [{formatted_scores}], Average: {np.mean(cv_scores):.2f}\")\n",
    "\n",
    "# Mapping of components to their optimal C values\n",
    "component_c_values = {\n",
    "    'left_eye': 0.1,\n",
    "    'right_eye': 0.1,\n",
    "    'nose_bridge': 10,\n",
    "    'full_nose': 50,\n",
    "    'left_eyebrow': 50,\n",
    "    'right_eyebrow': 1,\n",
    "    'mouth': 0.1\n",
    "}\n",
    "\n",
    "# Main execution block\n",
    "components = [\"left_eye\", \"right_eye\", \"nose_bridge\", \"full_nose\", \"left_eyebrow\", \"right_eyebrow\", \"mouth\"]\n",
    "db_path = 'face_features.db'\n",
    "\n",
    "for component in components:\n",
    "    # Retrieve the best C value for the current component\n",
    "    best_c = component_c_values[component]\n",
    "    print(f\"Validating {component} with C={best_c}\")\n",
    "    validate_svm_with_cross_validation(db_path, component, C=best_c, gamma='scale', kernel='rbf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_data = {\n",
    "    'Left_Eye': [0.62, 0.78, 0.76, 0.86, 0.39],\n",
    "    'Right_Eye': [0.59, 0.77, 0.76, 0.67, 0.42],\n",
    "    'Nose_Bridge': [0.90, 0.82, 0.63, 0.87, 0.89],\n",
    "    'Full_Nose': [0.66, 0.69, 0.56, 0.97, 0.87],\n",
    "    'Left_Eyebrow': [0.59, 0.73, 0.75, 0.87, 0.58],\n",
    "    'Right_Eyebrow': [0.78, 0.65, 0.77, 0.93, 0.12],\n",
    "    'Mouth': [0.64, 0.75, 0.87, 0.85, 0.66],\n",
    "}\n",
    "\n",
    "\n",
    "def plot_cross_validation_scores(data):\n",
    "    fig, ax = plt.subplots()\n",
    "    for component, scores in data.items():\n",
    "        sns.lineplot(x=[1, 2, 3, 4, 5], y=scores, label=component, marker='o')\n",
    "\n",
    "    ax.set_xlabel('Fold Number')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Cross-Validation Scores by Component')\n",
    "    ax.legend(title='Component')\n",
    "    plt.xticks([1, 2, 3, 4, 5])\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_cross_validation_scores(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# svm cross validation accuracies\n",
    "data = {\n",
    "    'Left_Eye': [0.62, 0.78, 0.76, 0.86, 0.39],\n",
    "    'Right_Eye': [0.59, 0.77, 0.76, 0.67, 0.42],\n",
    "    'Nose_Bridge': [0.90, 0.82, 0.63, 0.87, 0.89],\n",
    "    'Full_Nose': [0.66, 0.69, 0.56, 0.97, 0.87],\n",
    "    'Left_Eyebrow': [0.59, 0.73, 0.75, 0.87, 0.58],\n",
    "    'Right_Eyebrow': [0.78, 0.65, 0.77, 0.93, 0.12],\n",
    "    'Mouth': [0.64, 0.75, 0.87, 0.85, 0.66],\n",
    "}\n",
    "\n",
    "# Calculate averages and standard deviations\n",
    "averages = {k: np.mean(v) for k, v in data.items()}\n",
    "std_devs = {k: np.std(v) for k, v in data.items()}\n",
    "\n",
    "# Normalize these values to get a weighting scale\n",
    "max_avg = max(averages.values())\n",
    "weight_scale = {k: v / max_avg for k, v in averages.items()}\n",
    "\n",
    "# Print the results\n",
    "print(\"Averages:\", averages)\n",
    "print(\"Standard Deviations:\", std_devs)\n",
    "print(\"Weight Scale:\", weight_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculated weights\n",
    "weight_scale = {\n",
    "    'Left Eye': 0.829683698296837,\n",
    "    'Right Eye': 0.7810218978102189,\n",
    "    'Nose Bridge': 1.0,\n",
    "    'Full Nose': 0.9124087591240875,\n",
    "    'Left Eyebrow': 0.8564476885644767,\n",
    "    'Right Eyebrow': 0.7907542579075427,\n",
    "    'Mouth': 0.9172749391727495\n",
    "}\n",
    "\n",
    "# Calculate the total possible score\n",
    "total_possible_score = sum(weight_scale.values())\n",
    "\n",
    "# might be around 50% to 70% of the total score, depending on how strict\n",
    "threshold = 0.65 * total_possible_score\n",
    "\n",
    "print(\"Total Possible Score:\", total_possible_score)\n",
    "print(\"Threshold for 'authorized':\", threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_svm_models():\n",
    "    model_directory = \"saved_models\"\n",
    "    models = {}\n",
    "    components = ['left_eye', 'right_eye', 'nose_bridge', 'full_nose', 'left_eyebrow', 'right_eyebrow', 'mouth']\n",
    "    for component in components:\n",
    "        model_path = os.path.join(model_directory, f\"{component}_svm_model.pkl\")\n",
    "        if os.path.exists(model_path):\n",
    "            models[component] = joblib.load(model_path)\n",
    "        else:\n",
    "            models[component] = None  # Handle missing models\n",
    "    return models\n",
    "\n",
    "def get_svm_confidence(svm_model, feature):\n",
    "    return svm_model.decision_function([feature])\n",
    "\n",
    "def combine_component_outputs(combined_confidences, component_weights, svm_confidence_threshold):\n",
    "    total_score = 0\n",
    "    max_possible_score = sum(component_weights.values())  # Calculate the maximum possible score\n",
    "    for component, confidence in combined_confidences.items():\n",
    "        if confidence > svm_confidence_threshold:\n",
    "            total_score += confidence * component_weights.get(component, 0)\n",
    "            #should be between 0.5-0.7\n",
    "    return 'authorized' if total_score > 0.7 * max_possible_score else 'unauthorized'\n",
    "\n",
    "\n",
    "\n",
    "# Load models\n",
    "svm_models = load_svm_models()\n",
    "\n",
    "# Define component ranges and weights\n",
    "component_ranges = {\n",
    "    'left_eye': (36, 41), 'right_eye': (42, 47),\n",
    "    'left_eyebrow': (17, 21), 'right_eyebrow': (22, 26),\n",
    "    'nose_bridge': (27, 30), 'full_nose': (31, 35),\n",
    "    'mouth': (48, 60)\n",
    "}\n",
    "\n",
    "# Define component weights based on performance\n",
    "component_weights = {\n",
    "    'left_eye': 0.829683698296837, 'right_eye': 0.7810218978102189,\n",
    "    'nose_bridge': 1.0, 'full_nose': 0.9124087591240875,  # Increased for more reliable components\n",
    "    'mouth': 0.9172749391727495,\n",
    "    'left_eyebrow': 0.8564476885644767, 'right_eyebrow': 0.7907542579075427 \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"saved_models\"\n",
    "expected_components = ['left_eye', 'right_eye', 'nose_bridge', 'full_nose', 'left_eyebrow', 'right_eyebrow', 'mouth']\n",
    "\n",
    "# Check for each expected model file\n",
    "for component in expected_components:\n",
    "    model_path = os.path.join(model_directory, f\"{component}_svm_model.pkl\")\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Model file found: {model_path}\")\n",
    "    else:\n",
    "        print(f\"Model file missing: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "embedding_model = load_embedding_model()\n",
    "svm_models = load_svm_models()\n",
    "db_connection = sqlite3.connect('face_features.db')\n",
    "kd_tree = None\n",
    "user_ids = None\n",
    "number_of_dimensions = 2048\n",
    "database_path = '/Users/paulmitchell/Downloads/CS420_Project/face_features.db'\n",
    "\n",
    "\n",
    "def load_profiles(database_path):\n",
    "    \"\"\"Load stored profile embeddings and their user IDs from the database for authorized profiles,\n",
    "    and build a cKD-Tree incrementally. Only reloads if profiles are not already loaded.\"\"\"\n",
    "    global profiles, kd_tree, user_ids\n",
    "\n",
    "    # Check if profiles are already loaded\n",
    "    if profiles is not None and kd_tree is not None and user_ids is not None:\n",
    "        return profiles, kd_tree, user_ids\n",
    "\n",
    "    # Initialize local variables for loading process\n",
    "    profiles = {}\n",
    "    user_ids = []\n",
    "    tree = cKDTree(np.empty((0, number_of_dimensions)), balanced_tree=False)  # Start with an empty tree\n",
    "\n",
    "    try:\n",
    "        conn = sqlite3.connect(database_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT user_id, embedding FROM component_embeddings WHERE label = 1\")\n",
    "        results = cursor.fetchall()\n",
    "        \n",
    "        for user_id, embedding_str in results:\n",
    "            embedding = np.array(json.loads(embedding_str))\n",
    "            if embedding.size > 0:  # Ensure embedding is not empty\n",
    "                profiles[user_id] = embedding\n",
    "                user_ids.append(user_id)\n",
    "                # Dynamically add to cKDTree\n",
    "                tree = cKDTree(np.vstack([tree.data, embedding]), balanced_tree=False)\n",
    "        \n",
    "        conn.close()\n",
    "        kd_tree = tree  # Assign the built tree to the global variable\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return profiles, kd_tree, user_ids\n",
    "\n",
    "profiles, kd_tree, user_ids = load_profiles(database_path)\n",
    "\n",
    "\n",
    "def find_closest_profile(recognized_embeddings, tree, user_ids):\n",
    "    profile_votes = {}\n",
    "    for embedding in recognized_embeddings:\n",
    "        distance, index = tree.query(embedding, k=1)  # Querying for the closest match\n",
    "        if isinstance(index, np.ndarray):\n",
    "            profile_id = user_ids[index[0]]  # Handling as array if k>1\n",
    "        else:\n",
    "            profile_id = user_ids[index]  # Handling as scalar if k=1\n",
    "        \n",
    "        if profile_id in profile_votes:\n",
    "            profile_votes[profile_id] += 1\n",
    "        else:\n",
    "            profile_votes[profile_id] = 1\n",
    "\n",
    "    # Determine the profile with the most votes\n",
    "    max_votes = max(profile_votes.values(), default=0)\n",
    "    winning_profiles = [profile for profile, votes in profile_votes.items() if votes == max_votes]\n",
    "    return winning_profiles[0] if winning_profiles else \"Unrecognized\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '/Users/paulmitchell/Downloads/CS420_Project/face_features.db'\n",
    "database_path = '/Users/paulmitchell/Downloads/CS420_Project/face_features.db'\n",
    "batch_size = 10\n",
    "svm_models = load_svm_models()\n",
    "\n",
    "components = [\"left_eye\", \"right_eye\", \"nose_bridge\", \n",
    "              \"full_nose\", \"left_eyebrow\", \"right_eyebrow\", \"mouth\"]\n",
    "# Define component weights based on performance\n",
    "component_weights = {\n",
    "    'left_eye': 0.829683698296837, 'right_eye': 0.7810218978102189,  # Reduced or increased due to cross-val accuracies\n",
    "    'nose_bridge': 1.0, 'full_nose': 0.9124087591240875, \n",
    "    'mouth': 0.9172749391727495,\n",
    "    'left_eyebrow': 0.8564476885644767, 'right_eyebrow': 0.7907542579075427\n",
    "}\n",
    "component_ranges = {\n",
    "    'left_eye': (36, 41), 'right_eye': (42, 47),\n",
    "    'left_eyebrow': (17, 21), 'right_eyebrow': (22, 26),\n",
    "    'nose_bridge': (27, 30), 'full_nose': (31, 35),\n",
    "    'mouth': (48, 60)\n",
    "}\n",
    "\n",
    "def process_frames(frames, filename, frame_count, cap, svm_models, component_weights, svm_confidence_threshold, kd_tree, user_ids):\n",
    "    alert_summary = []\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)  # Get the frame rate from the video capture object\n",
    "    for frame in frames:\n",
    "        start_time = datetime.now()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray)\n",
    "\n",
    "        if not faces:  # Skip frames with no detected faces\n",
    "            continue\n",
    "\n",
    "        frame_timestamp = frame_count / frame_rate  # Calculate current timestamp based on frame count and frame rate\n",
    "        combined_confidences = {}\n",
    "        recognized_embeddings = []  # Initialize to ensure it's reset for each frame\n",
    "\n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray, face)\n",
    "            components = extract_components_from_face(gray, landmarks)\n",
    "\n",
    "            for component_name, component_img in components.items():\n",
    "                component_img = preprocess_input(component_img)  # Ensure preprocessing consistency\n",
    "                embedding = embedding_model.predict(np.expand_dims(component_img, axis=0))\n",
    "                if component_name in svm_models and svm_models[component_name] is not None:\n",
    "                    svm = svm_models[component_name]\n",
    "                    confidence = svm.decision_function(embedding)\n",
    "                    if confidence > svm_confidence_threshold:\n",
    "                        combined_confidences[component_name] = confidence\n",
    "                        recognized_embeddings.append(embedding)  # Store embedding for further recognition\n",
    "\n",
    "\n",
    "        if combined_confidences:  # Only process if there are valid confidences\n",
    "            final_decision = combine_component_outputs(combined_confidences, component_weights, svm_confidence_threshold)\n",
    "            if final_decision == 'authorized' and recognized_embeddings:\n",
    "                profile_name = \"Unrecognized\"  # Default to unrecognized\n",
    "                for embedding in recognized_embeddings:\n",
    "                    # Check each recognized embedding against profiles\n",
    "                    embedding = embedding.reshape(1, -1)  # Reshape for consistency if needed\n",
    "                    profile_name = find_closest_profile(embedding, kd_tree, user_ids)\n",
    "                    if profile_name != \"Unrecognized\":\n",
    "                        status = \"Recognized : \" + profile_name\n",
    "                        break\n",
    "            else:\n",
    "                status = \"Unrecognized\"\n",
    "                profile_name = \" \"\n",
    "\n",
    "            recognized_embeddings.clear()  # Clear the recognized embeddings after use\n",
    "\n",
    "            latency = (datetime.now() - start_time).total_seconds()\n",
    "            alert_summary.append(f\"{filename}, Person detected at: {frame_timestamp:.2f} seconds, {status}, Profile: {profile_name}, Confidence: {np.max(list(combined_confidences.values())) if combined_confidences else 0}, Latency: {latency:.2f}s\")\n",
    "\n",
    "    # Write to alert summary file\n",
    "    with open('alert_summary.txt', 'a') as f:\n",
    "        for line in alert_summary:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "    return len(frames)  # Return the number of processed frames to update the frame counter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_models = load_svm_models()\n",
    "svm_confidence_threshold = 0.7 \n",
    "video_directory = '/Users/paulmitchell/Downloads/CS420_Project/live_feed'\n",
    "batch_size = 10  # Define the number of frames to process at once\n",
    "\n",
    "# Loop through each video and process frames\n",
    "for filename in os.listdir(video_directory):\n",
    "    if filename.endswith('.mp4'):\n",
    "        video_path = os.path.join(video_directory, filename)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        frame_count = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "            if len(frames) == batch_size:\n",
    "                frame_count += process_frames(frames, filename, frame_count, cap, svm_models, component_weights, svm_confidence_threshold, kd_tree, user_ids)\n",
    "                frames = []  # Reset for next batch\n",
    "        if frames:  # Process any remaining frames\n",
    "            frame_count += process_frames(frames, filename, frame_count, cap, svm_models, component_weights, svm_confidence_threshold, kd_tree, user_ids)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def parse_confidences(file_path):\n",
    "    confidences = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split(',')\n",
    "            if 'Confidence' in parts[-2]:\n",
    "                confidence_score = float(parts[-2].split(':')[1].strip())\n",
    "                confidences.append(confidence_score)\n",
    "    return confidences\n",
    "\n",
    "# Load confidence scores\n",
    "confidences = parse_confidences('alert_summary.txt')\n",
    "\n",
    "# Calculate statistics\n",
    "mean_confidence = np.mean(confidences)\n",
    "median_confidence = np.median(confidences)\n",
    "std_deviation = np.std(confidences)\n",
    "\n",
    "print(f\"Mean Confidence: {mean_confidence}\")\n",
    "print(f\"Median Confidence: {median_confidence}\")\n",
    "print(f\"Standard Deviation: {std_deviation}\")\n",
    "\n",
    "# Plotting the histogram of confidence scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(confidences, bins=30, color='blue', alpha=0.7)\n",
    "plt.axvline(x=mean_confidence, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean_confidence:.2f}')\n",
    "plt.axvline(x=median_confidence, color='green', linestyle='dashed', linewidth=1, label=f'Median: {median_confidence:.2f}')\n",
    "plt.title('Histogram of SVM Confidence Scores')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define correct identification intervals for each video\n",
    "correct_intervals = {\n",
    "    'footage_1.mp4': [(0, 5, 'auth_face1'), (16, 21, 'unauthorized'), (26, 31, 'auth_face3'), (40, 45, 'auth_face2'), \n",
    "                      (47, 48, 'auth_face3'), (52, 57, 'unauthorized'), (59, 60, 'auth_face2')],\n",
    "    'footage_2.mp4': [(0, 1, 'unauthorized'), (7, 11, 'auth_face3'), (16, 21, 'auth_face2'), (24, 28, 'unauthorized')],\n",
    "    'footage_3.mp4': [(3, 7, 'auth_face3')],\n",
    "    'menoglasses_auth_face4.mp4': [(3, 8, 'auth_face4')],\n",
    "    'mewithglasses_auth_face4.mp4': [(1, 7, 'auth_face4')],\n",
    "    'footage_auth_face5.mp4': [(0, 21, 'auth_face5')],\n",
    "    'footage_auth_face6.mp4': [(0, 15, 'auth_face6')],\n",
    "    'footage_auth_face7.mp4': [(0, 13, 'auth_face7')],\n",
    "    'footage_auth_face8.mp4': [(0, 14, 'auth_face8')],\n",
    "}\n",
    "\n",
    "# Function to parse the alert summary\n",
    "def parse_alert_summary(file_path):\n",
    "    results = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split(',')\n",
    "            if len(parts) < 6:\n",
    "                continue\n",
    "            filename = parts[0].strip()\n",
    "            timestamp = float(parts[1].split(':')[1].strip().split(' ')[0])\n",
    "            status = parts[2].strip()\n",
    "            recognized = status.split(':')[1].strip() if 'Recognized' in status else 'unauthorized'\n",
    "            if filename not in results:\n",
    "                results[filename] = []\n",
    "            results[filename].append((timestamp, recognized))\n",
    "    return results\n",
    "\n",
    "alerts = parse_alert_summary('alert_summary.txt')\n",
    "\n",
    "# Function to calculate interval accuracy\n",
    "def calculate_interval_accuracy(alerts, intervals):\n",
    "    accuracies = {}\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for video, data in alerts.items():\n",
    "        video_accuracies = []\n",
    "        for start, end, identity in intervals.get(video, []):\n",
    "            interval_correct = []\n",
    "            for time, recognized in data:\n",
    "                if start <= time <= end:\n",
    "                    correct_recognition = (identity == recognized) or (identity == 'unauthorized' and recognized == 'Unrecognized')\n",
    "                    interval_correct.append(correct_recognition)\n",
    "                    y_true.append(identity)\n",
    "                    y_pred.append(recognized)\n",
    "            if interval_correct:\n",
    "                accuracy = sum(interval_correct) / len(interval_correct)\n",
    "                video_accuracies.append((start, end, accuracy))\n",
    "        accuracies[video] = video_accuracies\n",
    "    return accuracies, y_true, y_pred\n",
    "\n",
    "accuracies, y_true, y_pred = calculate_interval_accuracy(alerts, correct_intervals)\n",
    "\n",
    "# Compute confusion matrix\n",
    "labels = list(set(y_true + y_pred))  # Create a list of all possible labels\n",
    "conf_matrix = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, labels=labels))\n",
    "\n",
    "# Function to choose color based on accuracy\n",
    "def choose_color(accuracy):\n",
    "    if accuracy > 0.7:\n",
    "        return 'green'\n",
    "    elif accuracy > 0.5:\n",
    "        return 'orange'\n",
    "    else:\n",
    "        return 'red'\n",
    "\n",
    "# Plotting interval accuracies for each video\n",
    "for video, acc_data in accuracies.items():\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    if not acc_data:\n",
    "        print(f\"No data available for {video}.\")\n",
    "        plt.close(fig)\n",
    "        continue\n",
    "\n",
    "    for start, end, accuracy in acc_data:\n",
    "        color = choose_color(accuracy)\n",
    "        ax.barh([0], left=start, width=end-start, height=1, align='edge', color=color, alpha=0.5)\n",
    "        ax.text((start + end) / 2, 0, f\"{accuracy:.2f}\", ha='center', va='center', color='black')\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    max_time = max(x[1] for x in acc_data) if acc_data else 0\n",
    "    ax.set_xlim([0, max_time + 10])\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_title(f'Recognition Interval Accuracy for {video}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "with open('component_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(component_weights, f)\n",
    "\n",
    "with open('user_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(user_ids, f)\n",
    "\n",
    "async def send_embeddings_to_cloud(embeddings):\n",
    "    url = 'http://172.31.23.253:5000/process'\n",
    "    json_data = {comp: emb.tolist() if isinstance(emb, np.ndarray) else emb for comp, emb in embeddings.items()}\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url, json=json_data, timeout=100) as response:\n",
    "                return await response.json()\n",
    "        except asyncio.TimeoutError:\n",
    "            print(\"The request timed out\")\n",
    "            return None\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(\"HTTP Request failed:\", e)\n",
    "            return None\n",
    "\n",
    "async def process_video_files(video_directory):\n",
    "    for filename in os.listdir(video_directory):\n",
    "        if filename.endswith('.mp4'):\n",
    "            video_path = os.path.join(video_directory, filename)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = detector(gray)\n",
    "                for face in faces:\n",
    "                    landmarks = predictor(gray, face)\n",
    "                    embeddings = extract_components_from_face(gray, landmarks)\n",
    "                    start_time = datetime.now()\n",
    "                    result = await send_embeddings_to_cloud(embeddings)\n",
    "                    latency = (datetime.now() - start_time).total_seconds()\n",
    "                    alert_line = f\"{filename}, Person detected at: {datetime.now().timestamp():.2f} seconds, {result['status'] if result else 'Error'}, Profile: {result['profile_name'] if result else 'Error'}, Latency: {latency:.2f}s\\n\"\n",
    "\n",
    "                    with open('cloud_alert_summary.txt', 'a') as file:\n",
    "                        file.write(alert_line)\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "# Use this block if running in an environment with an existing event loop\n",
    "if __name__ == '__main__':\n",
    "    video_directory = '/Users/paulmitchell/Downloads/CS420_Project/live_feed'\n",
    "    loop = asyncio.get_event_loop()  # Get the existing event loop\n",
    "    try:\n",
    "        loop.run_until_complete(process_video_files(video_directory))  # Run the coroutine within the existing loop\n",
    "    finally:\n",
    "        loop.close()  # Close the loop when done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latencies(file_path):\n",
    "    latencies = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split(',')\n",
    "            if 'Latency' in parts[-1]:\n",
    "                latency_str = parts[-1].split(': ')[1].strip('s\\n')\n",
    "                try:\n",
    "                    latency = float(latency_str)\n",
    "                    latencies.append(latency)\n",
    "                except ValueError:\n",
    "                    continue  # Skip lines where latency could not be converted to float\n",
    "    return latencies\n",
    "\n",
    "def calculate_statistics(latencies):\n",
    "    mean = np.mean(latencies)\n",
    "    std_dev = np.std(latencies)\n",
    "    return mean, std_dev\n",
    "\n",
    "# Load latencies from both files\n",
    "original_latencies = extract_latencies('alert_summary.txt')\n",
    "modified_latencies = extract_latencies('cloud_alert_summary.txt')\n",
    "\n",
    "# Calculate statistics for both sets of latencies\n",
    "original_mean, original_std = calculate_statistics(original_latencies)\n",
    "modified_mean, modified_std = calculate_statistics(modified_latencies)\n",
    "\n",
    "# Perform a t-test to see if there's a significant difference\n",
    "t_stat, p_value = stats.ttest_ind(original_latencies, modified_latencies)\n",
    "\n",
    "print(f\"Edge File - Mean Latency: {original_mean:.2f}s, Std Dev: {original_std:.2f}\")\n",
    "print(f\"Cloud File - Mean Latency: {modified_mean:.2f}s, Std Dev: {modified_std:.2f}\")\n",
    "print(f\"T-statistic: {t_stat:.2f}, P-value: {p_value:.4f}\")\n",
    "\n",
    "# Determine if the difference is statistically significant\n",
    "if p_value < 0.05:\n",
    "    print(\"The difference in latencies is statistically significant.\")\n",
    "else:\n",
    "    print(\"No statistically significant difference in latencies.\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(original_latencies, bins=30, alpha=0.7, label='Edge Latencies')\n",
    "plt.hist(modified_latencies, bins=30, alpha=0.7, label='Cloud Latencies')\n",
    "plt.xlabel('Latency (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Comparison of Latencies')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
